{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "wLnOlwhACahl"
   },
   "source": [
    "Comparing Logistic regression with Transformers for Binary Email Classification(Ham,Spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "dfe4911de259415bafc68d27c45b5907",
      "fd683509a9b343a7b2a416893ca7f686",
      "59d683a9afe849d792f43c1c7460c2ad",
      "3d9f24a879e243bfba1091d6ef527851",
      "cef5d7c5871640dfabb02180e8f02e89",
      "2e66b405fca94dd685e55a884dca4f77",
      "7e70bf258ecb403490fd12be15eda5b1",
      "acc1eeff74114b85be51827463543769",
      "1ee2ba2cc240466fbd1ed097c7b9a96c",
      "2d7f49b29cc245e08997203bb02479d1",
      "cfa09dc5f2b545298ae462a292bcd06f",
      "c3eeb4213b8644e4821e0b377f5a0474",
      "761697cbab444a0280bcb9b79d653d1c",
      "3596b1c7bfaf47789b69258c74bd59e7",
      "6f87f228cedd40bbb661cd3051e3041d",
      "da8b157502d1435e89e850fbf0c991bf",
      "8b1c6681a36146f4a7690c1fb35bace8",
      "ae01c561f4934e25ad7f83bb975e5924",
      "c17db70228d8418ebb7175c554d8a8b0",
      "41bfa1817a3b45d185901094fb827c82",
      "a89f0ba6caaf44efbcb7f700e0a0f431",
      "d35cc1d4c8254b22bdf9ccf9ff423142",
      "5c567aac579d41028d4f4cde3a79fa07",
      "b454b07cd4b342699f49e19db94f5f69",
      "daf405d91562441c946cbb11f0823516",
      "4c17ae208bd24e33bc19c0e556a33824",
      "20d925b8b7da4ecd9788e49bd238b63e",
      "b3a0024cedb941fc99dab96eac33e0c9",
      "bf59f87cd10c4308b4d80198db516eca",
      "ec9a826de60b43fb8b626bb4b6e5122f",
      "d8265e2d3a2d4f9cb778de6ec9145b3f",
      "c9a24e33b28e47ada332b0e4e88ace1c",
      "3f83b58dfdb34854aad4e2d9e4e9f5b6"
     ]
    },
    "id": "iTGmpcG-HRmw",
    "outputId": "2528cbc9-f45d-4d43-99d6-4447a5099a45"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sms_spam\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82_fb6ahIkHK",
    "outputId": "775bdfc5-e4ce-4e0a-8f87-d9c27c4d8fbb"
   },
   "outputs": [],
   "source": [
    "print(dataset['train'][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TwuOSObWHvr",
    "outputId": "c7015c32-d9a6-462a-c80a-1b8c4cfe8403"
   },
   "outputs": [],
   "source": [
    "print(dataset['train'].features['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "hNVT_nU5ZRar"
   },
   "source": [
    "#investigating TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAGJ_jIrZYcp",
    "outputId": "ac3f8bfe-5b58-41f0-ee48-ab4163b6f1d0"
   },
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"I won the lottery\",\n",
    "    \"you won a lottery\",\n",
    "    \"Congratulations! we are happy to offer you SDE-1 role at Amazon JFK office\"\n",
    "]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 500)\n",
    "X = vectorizer.fit_transform(text)\n",
    "\n",
    "print(\"shappe of TF-IDF matrix:\", X.shape)\n",
    "print(\"Example of feature names:\", vectorizer.get_feature_names_out()[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "JkVjPZ0Rbm1d"
   },
   "source": [
    "Vectorizing the dataset! getting ready for that prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBK5uKWnblJa",
    "outputId": "8bf5362d-f6c8-41ad-bbe1-fd7b9e008b8f"
   },
   "outputs": [],
   "source": [
    "# doing the split with Hugging Face to avoid weird indexing issues\n",
    "splits = dataset['train'].train_test_split(test_size=0.2, seed=42, stratify_by_column='label')\n",
    "\n",
    "# pulling out the text and labels as plain lists (keeping it simple)\n",
    "train_texts = list(splits['train']['sms'])\n",
    "train_labels = list(map(int, splits['train']['label']))\n",
    "test_texts  = list(splits['test']['sms'])\n",
    "test_labels = list(map(int, splits['test']['label']))\n",
    "\n",
    "# vectorizing with tf-idf — small cap on features to stay fast\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# fitting on train only, then applying to test\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test  = vectorizer.transform(test_texts)\n",
    "\n",
    "# sanity check — rows = docs, cols = vocab size\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test  shape:\", X_test.shape)\n",
    "\n",
    "# quick peek at what words made it in\n",
    "print(\"Example features:\", vectorizer.get_feature_names_out()[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZztea4Xd1oX",
    "outputId": "55718cdc-74c5-481b-daf7-e7b740a3f522"
   },
   "outputs": [],
   "source": [
    "# training a simple baseline so I have a reference point\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fitting the model on the tf-idf features\n",
    "clf = LogisticRegression(max_iter=2000, n_jobs=-1)  # cranking up max_iter just in case\n",
    "clf.fit(X_train, train_labels)\n",
    "\n",
    "# getting predictions on the test set to see how well it generalizes\n",
    "test_pred = clf.predict(X_test)\n",
    "\n",
    "# quick metrics to get the vibe (accuracy + macro-F1 for imbalance)\n",
    "acc = accuracy_score(test_labels, test_pred)\n",
    "f1  = f1_score(test_labels, test_pred, average='macro')\n",
    "print(\"test accuracy:\", acc)\n",
    "print(\"macro-F1:\", f1)\n",
    "print(\"\\nclassification report:\\n\", classification_report(test_labels, test_pred, digits=3, target_names=[\"ham\",\"spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "N9Gmn5BPfVrH",
    "outputId": "963b2e03-0f33-44af-aaa9-af6d61205a44"
   },
   "outputs": [],
   "source": [
    "# drawing the confusion matrix to see *how* it’s making mistakes\n",
    "cm = confusion_matrix(test_labels, test_pred, labels=[0,1])  # 0=ham, 1=spam\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"ham\",\"spam\"])\n",
    "disp.plot(values_format=\"d\")\n",
    "plt.title(\"TF-IDF + LogisticRegression — Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "MTz_YZvwCS_m"
   },
   "source": [
    "Now trying this with AI to compare which method performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181,
     "referenced_widgets": [
      "c3d83fe335f44b95a530fd9d83edb14e",
      "99fee7d28cbf4be59a16d65833d0abe8",
      "12b3f4a5aaf4416fac72670aa5bf7e2a",
      "9a717aa614a047248498d55963504c4c",
      "f13ead5277e74933bc7efc69af519d67",
      "49fe2221bc26489aa08ce87fd852c4c0",
      "e36882729a0f4faa8a39a756171d4795",
      "e1d65a3e35fa476fab4003d0b4fecc59",
      "44b5d8bfdc60490dbf24934c328f66f3",
      "fd5a980dc260477aacd840a43619003b",
      "02438ca9a17a456c9f7641d57ef16cdd",
      "8033d61a3a594a53ab8d00bcb474211e",
      "6c748280f3e547eab03b22897e13d572",
      "a56cacd3a42841eeb86cf57a7cabf1b2",
      "0e0fcbd84ba5498d89ab40dcd9c61da1",
      "0e8994a6139543a4bb707200a38da36a",
      "df47aa103dbe455bacf8fb32b374ef84",
      "dfd1f3c8319c471ea624f19aea7fd3cd",
      "59aefb2c32c546d09291c2bcf0c2a071",
      "84c622aa247c4b9c91a6857753a06437",
      "ef9d5c34543247e7940c8dd9694400f4",
      "36c81f3caffd433a8c4fb59a93c841e6",
      "5795d259f04b4d72b959be0fcb3e2238",
      "80b6f841707c401a8da3a2ed26f0c196",
      "a1a001c03834424fb18c69222b5582b0",
      "2aa8d22f9f954a7582d71fb7ef87eb76",
      "b4adf54d65944d3d8603279b8a811e4b",
      "ba0f7919fe4d4639810b28121ebec57d",
      "6b9bff5e136247f79b4638a2eb56b292",
      "ea05b31905bf43ecac0b72738e2fea99",
      "1c956c4a7f5a4526917f5bb5e7d69742",
      "571957bfda9f4b868e325757f57725a8",
      "df40d63e891f4bae824ac1e65341ad96",
      "f0ebc4cdf53a43ceae5011ea5dca5220",
      "50dc3b2c0d814abf96a553546f4027e5",
      "17099d68d63b4804b71c12045a9fa4ea",
      "60bdf6635c794716a6022a530778a84a",
      "f326ffaa04764282b3fe6b64d3b25ccb",
      "d173221659324d4d9b543f2219849167",
      "63fa6fc2880e46c881fe32c129a41f44",
      "f8604eaddf4e41efbb9d6dc6df52bed2",
      "fa5c706ac77345e082a5daed53f42737",
      "c4b05e7546254e15b320d211b8818ecf",
      "bed7538a95f54ba1b65b7a17e0ddd1f5"
     ]
    },
    "id": "sRRqRsNBCpsb",
    "outputId": "0db1720e-c706-4f65-9681-ab2009e8e3a7"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "example = \"You won a lottery\"\n",
    "\n",
    "tokens = tokenizer.tokenize(example)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"IDs:\", ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "N0VY0W-pJSHp"
   },
   "source": [
    "***Using DistilBERT for classification. BERT is great at understanding text and next word predicition through self attention***"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "xl0uCYUiS8Oz"
   },
   "source": [
    "I am going to try out a single training step to first set up the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHzPFrvlTCxA",
    "outputId": "a97521d1-d3f8-4909-ba0d-a40f9d50fdb1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "c197d288f095474cbff1254689d75693",
      "79eb04c2760547cdae605d4cf040f09f",
      "ebce0d7f2f194082b26d8d435c57cfc6",
      "b470a26177764bcd99aecf0c021fc91b",
      "4ef4f8c5ec7f4cd681ca41d12a148228",
      "4dfd13d6c02949569049a0452cdef992",
      "da41dc4b504744ef942b905fd3385305",
      "79d517a66074406db879fa1e5a062bed",
      "e8915084e3634226a926b0be99ffbbb7",
      "e98461d0531d4abbbee372114644be47",
      "7c068ed228a44d118007af8cbc2dad16"
     ]
    },
    "id": "pBy4BybRWyr2",
    "outputId": "adb98e67-440b-45f0-9b8e-cfecf7c8c20d"
   },
   "outputs": [],
   "source": [
    "#setting up tokenizer-same as model\n",
    "tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels = 2,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7Pml4njXW6z",
    "outputId": "da3996bf-d51f-4e33-972f-b168fcd7cc8f"
   },
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Hey, are we still meeting at 3pm today?\",              # ham\n",
    "    \"WIN BIG! Claim your free prize now—limited offer!!!\"   # spam\"\n",
    "]\n",
    "labels = torch.tensor([0,1]).to(device)\n",
    "\n",
    "enc = tok(\n",
    "    text,\n",
    "    truncation = True,\n",
    "    padding = \"max_length\",\n",
    "    max_length = 16,\n",
    "    return_tensors = \"pt\"\n",
    ").to(device)\n",
    "# print(\"input_ids shape:\", enc[\"input_ids\"].shape)\n",
    "# print(\"attention_mask shape:\", enc[\"attention_mask\"].shape)\n",
    "# print(\"labels shape:\", labels.shape)\n",
    "print(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nv5KfZJjYvgk",
    "outputId": "9010cf76-c3ed-49e4-cf47-5398799d65c3"
   },
   "outputs": [],
   "source": [
    "out = model(**enc, labels=labels)\n",
    "loss = out.loss\n",
    "logits = out.logits\n",
    "\n",
    "print(\"loss:\", float(loss))\n",
    "print(\"logits shape:\", logits.shape)\n",
    "print(\"logits:\", logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "zhKvTUIzcwoN"
   },
   "source": [
    "trying to calculate the loss myself to confirm that we get the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8EnfEuwcI8N",
    "outputId": "eb7ca573-1a58-4298-817a-cdae8d39ef2d"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Cross-entropy expects raw logits and integer labels\n",
    "ce = F.cross_entropy(logits, labels, reduction=\"mean\")\n",
    "print(\"CE re-computed:\", float(ce.detach()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "lBo2Q8ZVc8Fm"
   },
   "source": [
    "now that we have the forward pass and loss, let’s do the backward pass and optimizer step — the moment where learning actually happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYOGFIRFc2Zb",
    "outputId": "9c232fc5-cedb-44c1-8ae1-36e4181fd82d"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "#activate dropouts\n",
    "model.train()\n",
    "#creating optimizer with model parameters\n",
    "opt = AdamW(model.parameters(),lr=2e-5)\n",
    "\n",
    "# 3) forward pass to get loss again\n",
    "out = model(**enc, labels=labels)\n",
    "loss = out.loss\n",
    "print(\"loss before:\", float(loss))\n",
    "\n",
    "# 4) zero old gradients\n",
    "opt.zero_grad()\n",
    "\n",
    "# 5) backward pass (calculate gradients)\n",
    "loss.backward()\n",
    "\n",
    "# 6) clip gradients (keeps them stable)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "# 7) optimizer step (update weights)\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "V3-sDIRilSNN"
   },
   "source": [
    "we track the loss after this one trainining example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KhnH5S6lYa-",
    "outputId": "f53cf843-b69d-4c2f-9e12-1518d93a9b71"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# evaluate mode for a clean measurement (no dropout)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out_after = model(**enc, labels=labels)\n",
    "    loss_after = out_after.loss\n",
    "    probs_after = F.softmax(out_after.logits, dim=-1)\n",
    "\n",
    "print(\"loss after:\", float(loss_after))\n",
    "print(\"probs after:\\n\", probs_after.cpu().numpy())  # rows = examples, cols = [ham, spam]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "61ZJ0P66slG8"
   },
   "source": [
    "the predictions are still ham. but the logits still shifted with the two examples which is great, it means with more data we can shift this to be more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "h-qYLKhZsh97"
   },
   "source": [
    "Now we are going to replicate this training loop but with a full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwe7_Fr-shoF",
    "outputId": "f83d995e-8401-442b-8151-b5f02489bf7e"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load sms_spam dataset\n",
    "dataset = load_dataset(\"sms_spam\")\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])  # peek at the first example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "snIaCYBMzGmC",
    "outputId": "e272f40f-b4f0-477a-ff51-4ac278e265bd"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "# stratified 80/20 split so class balance is preserved\n",
    "splits = dataset[\"train\"].train_test_split(\n",
    "    test_size=0.2, seed=42, stratify_by_column=\"label\"\n",
    ")\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds   = splits[\"test\"]\n",
    "\n",
    "# quick sanity checks\n",
    "print(\"train size:\", len(train_ds), \"val size:\", len(val_ds))\n",
    "\n",
    "# show label names if available (0=ham, 1=spam)\n",
    "labels_feat = train_ds.features[\"label\"]\n",
    "if isinstance(labels_feat, ClassLabel):\n",
    "    print(\"label names:\", labels_feat.names)\n",
    "\n",
    "# tiny peek at one of each split\n",
    "print(\"train ex:\", train_ds[0])\n",
    "print(\"val ex:\", val_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLP9lqKm3TR3",
    "outputId": "49081043-aa69-4f8e-bb09-a52c7b995f39"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [x[\"sms\"] for x in batch]\n",
    "    labels = [x[\"label\"] for x in batch]\n",
    "    enc = tok(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    enc[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "    return enc.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# sanity check: grab one batch\n",
    "batch = next(iter(train_loader))\n",
    "print({k: v.shape for k, v in batch.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "_iBkxsVkXD8p"
   },
   "source": [
    "mini train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfcQ86gR6Ebx",
    "outputId": "abccf9e2-0580-455f-dca8-3e086aa7c12b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 1) model (distilbert + 2-class head) on the same device\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ").to(device)\n",
    "\n",
    "# 2) optimizer\n",
    "opt = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 3) one short epoch over train data\n",
    "model.train()\n",
    "running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "for step, batch in enumerate(train_loader):\n",
    "    # forward\n",
    "    out = model(**batch)                 # batch has input_ids, attention_mask, labels\n",
    "    loss = out.loss                      # scalar loss\n",
    "    logits = out.logits                  # [B, 2]\n",
    "    preds = logits.argmax(dim=-1)        # [B]\n",
    "\n",
    "    # metrics\n",
    "    running_loss += loss.item() * preds.size(0)\n",
    "    correct += (preds == batch[\"labels\"]).sum().item()\n",
    "    total += preds.size(0)\n",
    "\n",
    "    # backward\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    opt.step()\n",
    "\n",
    "    # keep it short for now (just ~200 examples), then we evaluate\n",
    "    if (step+1) * train_loader.batch_size >= 200:\n",
    "        break\n",
    "\n",
    "train_loss = running_loss / total\n",
    "train_acc  = correct / total\n",
    "print(f\"train: n={total}, loss={train_loss:.4f}, acc={train_acc:.3f}\")\n",
    "\n",
    "# 4) quick validation pass\n",
    "model.eval()\n",
    "val_correct, val_total, val_loss_sum = 0, 0, 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        out = model(**batch)\n",
    "        val_loss_sum += out.loss.item() * batch[\"labels\"].size(0)\n",
    "        preds = out.logits.argmax(dim=-1)\n",
    "        val_correct += (preds == batch[\"labels\"]).sum().item()\n",
    "        val_total += batch[\"labels\"].size(0)\n",
    "\n",
    "val_loss = val_loss_sum / val_total\n",
    "val_acc  = val_correct / val_total\n",
    "print(f\"val:   n={val_total}, loss={val_loss:.4f}, acc={val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEWTp80sXF2I",
    "outputId": "d2ef972d-ca27-43c3-b4ee-3e21e18c26cd"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# fresh model (optional). comment this line out if you want to continue training the same model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2).to(device)\n",
    "\n",
    "opt = AdamW(model.parameters(), lr=2e-5)\n",
    "# simple linear warmdown scheduler (keeps things stable)\n",
    "scheduler = LinearLR(opt, start_factor=1.0, end_factor=0.1, total_iters=3)  # 3 epochs\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            out = model(**batch)\n",
    "            loss_sum += out.loss.item() * batch[\"labels\"].size(0)\n",
    "            preds = out.logits.argmax(dim=-1)\n",
    "            correct += (preds == batch[\"labels\"]).sum().item()\n",
    "            total += batch[\"labels\"].size(0)\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for batch in train_loader:\n",
    "        out = model(**batch)\n",
    "        loss = out.loss\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        # stats\n",
    "        preds = out.logits.argmax(dim=-1)\n",
    "        loss_sum += loss.item() * preds.size(0)\n",
    "        correct += (preds == batch[\"labels\"]).sum().item()\n",
    "        total += preds.size(0)\n",
    "\n",
    "    train_loss, train_acc = loss_sum/total, correct/total\n",
    "    val_loss, val_acc = evaluate()\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"epoch {epoch}: train loss {train_loss:.4f} acc {train_acc:.3f} | val loss {val_loss:.4f} acc {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "5tm_lVn-YePF"
   },
   "source": [
    "The loss has significantly reduced now with the 10 epochs i run from the 0.5 to 0.05 which is hige on acuracy. it is getting to 99 percent accuracy also which is already better than logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "PETf0LKPYx4y",
    "outputId": "60c2ba81-d254-4b63-9e6b-a83a40edd8f6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# collect all predictions on the validation set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        out = model(**batch)\n",
    "        preds = out.logits.argmax(dim=-1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().tolist())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1])  # 0=ham, 1=spam\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"ham\",\"spam\"])\n",
    "disp.plot(values_format=\"d\")\n",
    "plt.title(\"Ham/Spam — Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZcOpgVgkjj5",
    "outputId": "c0857d15-9b4f-45d6-c1c2-67d451cf1abe"
   },
   "outputs": [],
   "source": [
    "# save\n",
    "save_dir = \"distilbert-hamspam-manual\"\n",
    "model.save_pretrained(save_dir)\n",
    "tok.save_pretrained(save_dir)\n",
    "print(\"saved to:\", save_dir)\n",
    "\n",
    "# load later (new objects)\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tok2 = AutoTokenizer.from_pretrained(save_dir)\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(save_dir).to(device)\n",
    "\n",
    "# quick sanity check on a sample\n",
    "import torch.nn.functional as F\n",
    "text = \"WIN BIG! Claim your prize now!\"\n",
    "enc = tok2(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64).to(device)\n",
    "with torch.no_grad():\n",
    "    probs = F.softmax(model2(**enc).logits, dim=-1).cpu().numpy()[0]\n",
    "print(\"probs [ham, spam]:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285,
     "referenced_widgets": [
      "a9879377090247adb7c82ef9586305e0",
      "b6b660b6a653485fb0e4d949b8bd7ff9",
      "6038ccc3dd3c48b2ba2ef0d86ebb628d",
      "dbcaa32d2db34d359be41471713c3bcc",
      "94494b9d02bb4392beefd44fa2344d8c",
      "8038b45371704e4189547a2b90583446",
      "7e1c394bede7427f9776ab0aa8ad6a68",
      "6535ee79e0334475af8becf15dce08e1",
      "db9469d51f2047758bc5ceae670ec8fb",
      "72782befb2f947a88332a101d9ecc7fa",
      "1c776d139b96407884c01ea226455814",
      "e52acfa83b574e56a12c03fd36c37599",
      "710afaaff2e44d4faa7a50d5456d4549",
      "34906858c52445e1876a916d33c4f64f",
      "38ee8fb86e614580a9e437fe2925b4a3",
      "ce7d609bb2f642ee8a8433b2806e2451",
      "682d908a9ee74c87af7c23be8e0eb64e",
      "10f467cfe2c94b5a9e23f0e866695674",
      "3cf50c64d6bb47629e3e97c7b17a6df5",
      "57748c05a39c4d1094b20f6db2d2cb8c"
     ]
    },
    "id": "MzPRnIVQq59Y",
    "outputId": "c67031c4-2e02-4cec-b54f-3747542928cc"
   },
   "outputs": [],
   "source": [
    "# 1) Make sure the hub library is up to date\n",
    "%pip install -U huggingface_hub\n",
    "\n",
    "# 2) Clear any cached/incorrect credentials (optional but helpful)\n",
    "from huggingface_hub import logout\n",
    "logout()  # ignores if you weren't logged in\n",
    "\n",
    "# 3) Login with an access token (NOT your password)\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()  # paste your HF token here (Settings -> Access Tokens -> New token with \"Write\" permission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0krC0ucq8Qi",
    "outputId": "4a4e8c10-2d0a-4b65-d0a9-3f89d3c2dbdd"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "whoami()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131,
     "referenced_widgets": [
      "24c1ec35eb9548e2a92356e3cbc18c8f",
      "2579f3427c74440c8c43d0d5a94b4a8e",
      "41926625a3094e9a8304a3ecafe53430",
      "9bb8f73cd966453bbfe2553b381ba120",
      "831665be0016497a9ff832683ea86ada",
      "b0364fd9225443dc9ce87ce4daab2bfe",
      "788e2722d3544571aaf0cfe8dbd2cce2",
      "c36b51a3be864bdf834ba051bfc6428a",
      "8912336ebbe44cfd9821b05fe72b284b",
      "6f84a39af0d44ff481321438b4f5e0b0",
      "ae96639520bb4a1bbe551fe05d6684d9",
      "a1ba51e5a11c4d66b41b8fde7efe5531",
      "ad4ef5d048aa46de9cf573c8f6cd9520",
      "d06faec84c4b4675b3c8d535d2a8a110",
      "b93bbea128e846109418d7b4ef0101c6",
      "134e07f586724120b174464a3094521f",
      "4f0422be21754373a4126a785647ff87",
      "42c7941b044b4dedb8e859330c42c81c",
      "19d08426d46e47349a0c8f4222771c31",
      "466e179af1d3498a8673ca0c27e94461",
      "7592115009534bb094f14da4c6c990b6",
      "075ce00881d646918cc3bbd316ddbb02",
      "3a0bad9272074f71a08d0f6168a7499a",
      "b61fbd2d7944483e8705aac2341542c7",
      "b3156a660b30475d931ebd5dfe129270",
      "7559ce562aab4bb3ac8d162d2805ccbd",
      "c4169871cb4b41beb1051be233f5819e",
      "46e462cf18da4b9fa4ad233a3aa2591c",
      "81c5f95976124ca699eb05e5eb625e97",
      "3b405f6e1bab4e618316c557f43bfad9",
      "bb3d66b0ae284bb4bb11713dba8f120f",
      "664b5a3ea988476596b6164be3a35195",
      "4a5ebfa714ea41e0b8614afec0860e00"
     ]
    },
    "id": "vZpKgTWLrPvb",
    "outputId": "703af155-3b1f-4311-e636-af4cbc3010b6"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "repo_id = \"AgnessLungu/distilbert-hamspam\"     # you can rename 'distilbert-hamspam' if you prefer\n",
    "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
    "\n",
    "# IMPORTANT: this path must match the folder you saved earlier (the one with config.json, tokenizer.json, model.safetensors/bin)\n",
    "folder_path = \"distilbert-hamspam-manual\"      # <-- change if your saved folder name is different\n",
    "\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=folder_path,\n",
    "    commit_message=\"Initial model upload from Colab\"\n",
    ")\n",
    "\n",
    "print(\"✅ Pushed to:\", f\"https://huggingface.co/{repo_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1uycLRgupKj",
    "outputId": "7c3ad0c3-7af7-4b26-f454-39e65977414c"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "import os\n",
    "\n",
    "space_id = \"AgnessLungu/ham-spam-ui\"           # keep or rename\n",
    "model_id = \"AgnessLungu/distilbert-hamspam\"    # your model repo\n",
    "\n",
    "# (Re)create the local app files if needed\n",
    "os.makedirs(\"space_app\", exist_ok=True)\n",
    "\n",
    "app_py = f\"\"\"\n",
    "import gradio as gr\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_ID = \"{model_id}\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "labels = getattr(model.config, \"id2label\", {{0: \"ham\", 1: \"spam\"}})\n",
    "\n",
    "def predict(text):\n",
    "    if not text or not text.strip():\n",
    "        return {{labels.get(0, \"ham\"): 1.0, labels.get(1, \"spam\"): 0.0}}\n",
    "    enc = tok(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "    return {{labels[0]: float(probs[0]), labels[1]: float(probs[1])}}\n",
    "\n",
    "with gr.Blocks(title=\"Ham vs Spam\") as demo:\n",
    "    gr.Markdown(\"# Ham vs Spam\\\\nPaste an email and click **Classify**.\")\n",
    "    inp = gr.Textbox(lines=8, label=\"Email text\")\n",
    "    out = gr.Label(label=\"Prediction (ham/spam)\")\n",
    "    gr.Button(\"Classify\").click(fn=predict, inputs=inp, outputs=out)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "\"\"\"\n",
    "with open(\"space_app/app.py\", \"w\") as f:\n",
    "    f.write(app_py)\n",
    "\n",
    "with open(\"space_app/requirements.txt\", \"w\") as f:\n",
    "    f.write(\"transformers>=4.40\\ntorch\\ngradio>=4.0\\n\")\n",
    "\n",
    "# 1) Create the Space (repo_type='space' + sdk='gradio')\n",
    "create_repo(space_id, repo_type=\"space\", space_sdk=\"gradio\", exist_ok=True)\n",
    "\n",
    "# 2) Upload the folder to the Space (IMPORTANT: repo_type='space')\n",
    "upload_folder(\n",
    "    repo_id=space_id,\n",
    "    repo_type=\"space\",                   # <-- this fixes the 404\n",
    "    folder_path=\"space_app\",\n",
    "    commit_message=\"Initial UI\"\n",
    ")\n",
    "\n",
    "print(\"✅ Space created/updated:\", f\"https://huggingface.co/spaces/{space_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdamrMj4_G8y",
    "outputId": "516fc7cc-567c-4ee2-9b39-c269c97becf0"
   },
   "outputs": [],
   "source": [
    "# Define human-readable labels\n",
    "id2label = {0: \"ham\", 1: \"spam\"}\n",
    "label2id = {\"ham\": 0, \"spam\": 1}\n",
    "\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "# save again with these mappings\n",
    "model.save_pretrained(\"distilbert-hamspam-manual\")\n",
    "tok.save_pretrained(\"distilbert-hamspam-manual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184,
     "referenced_widgets": [
      "6726b72fa06d42948c51bd8c9269be72",
      "9d46f4937fc548a6a2bcfac834498b17",
      "af1301eab73640a993432560dcd00454",
      "805bbff941074a5fac32cbe767c7b012",
      "5ba019d6542d489e94bc30a4b09c4eed",
      "f2a58adfe5544a2eb38625f7f1d71c67",
      "ecb9f5cebaf545f8b28ced1961bfc475",
      "54998734a3ed425bbff94ec93ed2cf59",
      "3eddc26e585f4179980f8ae42974221a",
      "d68e5915f0214eefbabcf07ce6e5e105",
      "a38437b8096a4912bcc5a386d707dd96",
      "02a4d4e0778c439db99b56c03111950d",
      "84bfd825ab96482680b46b6a6be318a4",
      "19d8a9c514c24149842ad800d6792dc1",
      "51fa7d56d091483b980a0d8b7e5e5e9b",
      "8fda09513828425a97ed5261416a39af",
      "6a0e51f699d84e5f826ce97c0416fa0e",
      "7a4c64aa26064d96b003920cbc9b2e45",
      "eff5ea45ca4c438eb083cb48a9b10eeb",
      "56e8b29ac00f41768a602ebc315840c5",
      "8ec725fe0a3d4663bd8b3c225f3ffb5b",
      "96964ed28d7e4136880716d5ecd52042",
      "4e256bdde28a42fbbf7db43bf8d3dad4",
      "79fce80e1d744d02bd477375dc963833",
      "f73695248d54416aae92cf33103ddda3",
      "70d8e81aa822404485a47df41441839f",
      "2882ad46db884505b3d905a55ecad06b",
      "4663b1a912db40cd9eab2f59b72fa3de",
      "60cf0ddca8f34e299b2ec06768df36fc",
      "7709afb6f83144438d839a628a009bba",
      "6e06815f5caa4350958350af5d9fb445",
      "067250404dfc40ddaac80d44a1d01609",
      "75054daa0ad340a5ba511e8bc1e38e4a"
     ]
    },
    "id": "T1TqZ9ZECEx7",
    "outputId": "04a74217-0ac7-44ae-a1b3-7a098fb4ffc6"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "\n",
    "upload_folder(\n",
    "    repo_id=\"AgnessLungu/distilbert-hamspam\",\n",
    "    folder_path=\"distilbert-hamspam-manual\",\n",
    "    commit_message=\"Add ham/spam labels\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "04dde7a8f2ca426a8be583803679b344",
      "cf8632f300e0401e956707da011bcd7c",
      "9aeaa16e8bac46d9bbe9d25ecccd11b5",
      "66585bc51ae74830bd635a3e30a9f6e2",
      "c78d2ee6aee9417782f5494e5035b232",
      "2f31230693eb4b69a948bdf240ed7463",
      "39ced37b981042fd9cb2999e41d64177",
      "5d3a91460f284c4db57b4d5302232c1e",
      "c64fcb9f70834318ab94755cd951ac6f",
      "44b2eea0cd5e4547b2b6abd78e66869a",
      "26a2e35cd9b049cb8c5494266285d5fb",
      "03da4e7d877843daad66f7531209dccf",
      "857ba4f27c8a4c9791ea4e56d87675d0",
      "9f30161b61fe4ddc87b08a5dd199755a",
      "8e2720f218ce4f3b8df8b7a99f1f700c",
      "9549bce3b7d143ac8565794b3d740b3a",
      "8fc1865ac7584c259745fbc7a5bd50f9",
      "4ea1871476594af8a233da9457396f99",
      "e715d350da814ffab8b8ee61e99375a4",
      "96a6185f853f442fb3f4725b18e624ef",
      "1da46f18d6444d22bfb14fdddd2b6dea",
      "7323590bda7d4113b7d7cc480e41fada",
      "8ae2b7b6483e4e8ba0f885fc8371c634",
      "8be02213e2014616830df4ac91ef5d7e",
      "9ead4fe39a424f69b10fda8b5507bdf3",
      "2ad481c069c24758ba342d1c42553b57",
      "6a4fb245f15f4bbe9a9381906e2fe928",
      "ce3c401e703e4c539a69509c7aca09f1",
      "43abc5888bfd4d518749c2e6d802420d",
      "941027c6ac8c4bc69c410b384c45e212",
      "62a6df8d999b4449b95c1d487ff83f14",
      "32752084a7c948869708d9530c60020b",
      "2441ebc27e0244e8a1743cf7a02e6f14"
     ]
    },
    "id": "Ilynuq2kDRfU",
    "outputId": "de5d44d5-2675-4187-b1d3-4c2b1bde76a4"
   },
   "outputs": [],
   "source": [
    "# run this once in your training notebook, then re-upload the folder\n",
    "id2label = {0: \"ham\", 1: \"spam\"}\n",
    "label2id = {\"ham\": 0, \"spam\": 1}\n",
    "\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "model.save_pretrained(\"distilbert-hamspam-manual\")\n",
    "tok.save_pretrained(\"distilbert-hamspam-manual\")\n",
    "\n",
    "from huggingface_hub import upload_folder\n",
    "upload_folder(\n",
    "    repo_id=\"AgnessLungu/distilbert-hamspam\",\n",
    "    folder_path=\"distilbert-hamspam-manual\",\n",
    "    commit_message=\"Add id2label/label2id (ham/spam)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "jHb-PbtwDmz4",
    "outputId": "41e86935-e429-48ee-8c9e-a0d70602ec65"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "upload_folder(\n",
    "    repo_id=\"AgnessLungu/ham-spam-ui\",\n",
    "    repo_type=\"space\",\n",
    "    folder_path=\"space_app\",\n",
    "    commit_message=\"Show human-readable labels and confidence\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
